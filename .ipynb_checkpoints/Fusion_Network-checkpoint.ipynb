{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3466a53d-321a-4264-aec6-7df53ec0ab5d",
   "metadata": {},
   "source": [
    "# **S**pectroscopy of **L**iquids **U**sing **R**obot **P**erception \n",
    "# *(SLURP)* Data Analysis\n",
    "\n",
    "### Author: Nathaniel Hanson\n",
    "### Last Updated: 08/28/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53a77e-d38c-4962-a64a-9f3d79c800e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main imports\n",
    "import os\n",
    "import csv\n",
    "import typing\n",
    "import os\n",
    "import torch\n",
    "import pickle as pk\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import numpy.matlib\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import SpectralData, train_epochs\n",
    "from models import MLP, Simple1DCNN, FusedNet, ContNet\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, LeaveOneOut\n",
    "# Change default figure size depending on display\n",
    "matplotlib.rcParams['figure.figsize'] = [10, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e168a80-be2e-49f3-bd90-6f3d78e1cd96",
   "metadata": {},
   "source": [
    "### Part 0: Read in all spectrometer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1adc8f-a42f-4d08-b612-a5a1f5494a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all data\n",
    "read_files = os.listdir('data')\n",
    "readings = {z.split('.')[0]:pd.read_csv(os.path.join('./data',z)) for z in read_files}\n",
    "# Remove extra data fields from the readings\n",
    "for key in readings.keys():\n",
    "    try:\n",
    "        # print(list(readings[key].columns)[:-4])\n",
    "        readings[key] = readings[key].loc[:,list(readings[key].columns)[:-5]].to_numpy().astype(np.int32)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    # print(readings[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2780b4b-b253-4946-8608-bc125b587db3",
   "metadata": {},
   "source": [
    "### Perform reflectance calibraiton across all data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c194e-a438-4350-8ced-02d4b068e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in calibration data\n",
    "hamamatsu_dark = np.median(pd.read_csv('./calibration/hamamatsu_black_ref.csv').to_numpy().astype(np.int32), axis=0)\n",
    "hamamatsu_white = np.median(pd.read_csv('./calibration/hamamatsu_white_ref.csv').to_numpy().astype(np.int32), axis=0)\n",
    "mantispectra_dark = np.median(pd.read_csv('./calibration/mantispectra_black_ref.csv').to_numpy()[:,:-5].astype(np.int32), axis=0)\n",
    "mantispectra_white = np.median(pd.read_csv('./calibration/mantispectra_white_ref.csv').to_numpy()[:,:-5].astype(np.int32), axis=0)\n",
    "\n",
    "# Create composite calibration file\n",
    "white_ref = np.concatenate((hamamatsu_white, mantispectra_white))\n",
    "dark_ref = np.concatenate((hamamatsu_dark, mantispectra_dark))\n",
    "\n",
    "# Create calibration function\n",
    "def spectral_calibration(reading):\n",
    "    t = np.divide((reading-dark_ref), (white_ref-dark_ref), where=(white_ref-dark_ref)!=0)\n",
    "    # Handle cases where there is null division, which casts values as \"None\"\n",
    "    if np.sum(t==None) > 0:\n",
    "        print('Null readings!')\n",
    "    t[t== None] = 0\n",
    "    # Handle edge cases with large spikes in data, clip to be within a factor of the white reference to avoid skewing the model\n",
    "    t = np.clip(t,-2,2)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce29564-9780-4bda-90a8-994a7b3dde5b",
   "metadata": {},
   "source": [
    "### Reflectance normalize spectral readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48537f9e-4473-4440-ab16-5cddf18b6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate all the data\n",
    "readings_cal = {}\n",
    "for key in readings.keys():\n",
    "    readings_cal[key] = np.apply_along_axis(spectral_calibration,1,readings[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a57357d-e40b-43c6-811b-f0a73dff9524",
   "metadata": {},
   "source": [
    "### Read label data for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd38ed7-4239-41e0-9b72-0e771234e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the container-substrate pairings\n",
    "pairings = pd.read_csv('./container_substrate.csv',header=1, keep_default_na=False)\n",
    "# Remove blank data rows\n",
    "pairings = pairings.loc[:18,(pairings.columns)[:20]]\n",
    "# Unique substances\n",
    "contents = list(pairings.columns[1:])\n",
    "# display(pairings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ca083-33ec-41f3-9c9c-ae175c386b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e34b1-b55e-451d-8716-548c65cf9c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.default_rng().normal(1.0,0.05,1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4038446-c7d5-4686-afaa-ba5dfb9f251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Containers to exclude - wood, stainless steel, aluminum\n",
    "exclude_containers = ['P','Q','R','J']\n",
    "exclude_contents = [17,18,2,12,0,9]\n",
    "# Generalized function to group data by the contents type\n",
    "\n",
    "def random_scale(reading: np.array) -> np.array:\n",
    "    return reading * np.random.default_rng().normal(1.0,0.05,1)[0]\n",
    "\n",
    "def generate_data_labels(readings: Dict) -> defaultdict:\n",
    "    data_by_contents = np.array([])\n",
    "    labels_by_contents = np.array([])\n",
    "\n",
    "    # Iterate over all data_frames types\n",
    "    for key in readings.keys():\n",
    "        # Iterate over all containers, but skip Aluminum (P), Stainless Steel (Q), and Wood (R)\n",
    "        if key[0] in exclude_containers or (len(key) > 1 and int(key[1:]) in exclude_contents): #:or int(key[1:]) in exclude_contents:\n",
    "            print(key)\n",
    "            continue\n",
    "        for index, val in enumerate(contents):\n",
    "            if key not in list(pairings[val]):\n",
    "                continue\n",
    "            # Otherwise the data is useful to use, let's proceed with the data wrangling\n",
    "            useData = readings[key]\n",
    "            # ADD SCALING NOISE TO THE DATA HERE\n",
    "            useData = np.matlib.repmat(useData,3,1)\n",
    "            useData = np.apply_along_axis(random_scale,1,useData)\n",
    "            # Get the plain name of the container\n",
    "            useContainer = pairings[np.equal.outer(pairings.to_numpy(copy=True),  [key]).any(axis=1).all(axis=1)]['container / substrate'].iloc[0]\n",
    "            # Add the index as the key value\n",
    "            data_by_contents = np.vstack((data_by_contents, useData)) if data_by_contents.size else useData\n",
    "            labels_by_contents = np.vstack((labels_by_contents, np.matlib.repmat([val,useContainer],useData.shape[0],1))) if labels_by_contents.size else np.matlib.repmat([val,useContainer],useData.shape[0],1)\n",
    "    return data_by_contents, labels_by_contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea91c356-5a76-4d2c-a781-8a897528f2f9",
   "metadata": {},
   "source": [
    "### Extract features from signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb15084-7516-45f7-a6a8-148d83708627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(input_size: int, num_containers: int, num_contents: int, hidden_layers_l1: Tuple=(256,128), hidden_layers_l2: Tuple=(256,128)) -> Tuple[nn.Module,str]:\n",
    "    '''\n",
    "    Generate a train able neural network architecture\n",
    "    \n",
    "    Args: \n",
    "        architecture (str): Type of architecture to generate\n",
    "        input_size (int): Number of input features\n",
    "        classes (int): Number of output features (classes)\n",
    "        hidden_layers (Tuple, optional): Size of sequential hidden layers to use in network construction\n",
    "        \n",
    "    Returns:\n",
    "        nn.Module: Model architecture\n",
    "    '''\n",
    "\n",
    "    # find device to train on\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f'Using {device} device')\n",
    "\n",
    "    model = ContNet(input_size, num_containers, num_contents, hidden_layers_l1, hidden_layers_l2)\n",
    "    \n",
    "    return model, device\n",
    "\n",
    "def train_val_test_split(all_data: np.ndarray, targets: np.ndarray) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    '''\n",
    "    Create train-test-val split from model data\n",
    "    \n",
    "    Args: \n",
    "        all_data (np.ndarray): 2D matrix of data to analyze\n",
    "        targets (list): Numerical categorical labels for each row of data\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[DataLoader,DataLoader,DataLoader]: Train dataloader; validation dataloader; test dataloader\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_data.astype(np.double), targets, test_size=0.10, stratify=targets)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, stratify=y_train) # 0.25 x 0.8 = 0.2\n",
    "    train_dataset = TensorDataset(torch.Tensor(X_train),torch.Tensor(y_train).type(torch.LongTensor)) # Main train set\n",
    "    valid_dataset = TensorDataset(torch.Tensor(X_val),torch.Tensor(y_val).type(torch.LongTensor))  # Validation set\n",
    "    test_dataset = TensorDataset(torch.Tensor(X_test),torch.Tensor(y_test).type(torch.LongTensor))  # Test set\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=256, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=valid_dataset, batch_size=256, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=256, shuffle=True)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_val_test_split_cross(all_data: np.ndarray, targets: np.ndarray) -> Tuple[TensorDataset, TensorDataset, DataLoader]:\n",
    "    '''\n",
    "    Create train-test-val split from model data for cross validation model training\n",
    "    \n",
    "    Args: \n",
    "        all_data (np.ndarray): 2D matrix of data to analyze\n",
    "        targets (list): Numerical categorical labels for each row of data\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[TensorDataset,TensorDataset,DataLoader]: Train dataset; validation dataset; test dataloader\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_data.astype(np.double), targets, test_size=0.2, stratify=targets)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train) # 0.25 x 0.8 = 0.2\n",
    "    train_dataset = TensorDataset(torch.Tensor(X_train),torch.Tensor(y_train).type(torch.LongTensor)) # Main train set\n",
    "    valid_dataset = TensorDataset(torch.Tensor(X_val),torch.Tensor(y_val).type(torch.LongTensor))  # Validation set\n",
    "    test_dataset = TensorDataset(torch.Tensor(X_test),torch.Tensor(y_test).type(torch.LongTensor))  # Test set\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=False)\n",
    "    return train_dataset, valid_dataset, test_loader\n",
    "\n",
    "def reset_weights(m: nn.Module):\n",
    "    '''\n",
    "    Try resetting model weights to avoid weight leakage  \n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Network to reset weight on\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "\n",
    "        \n",
    "def train_model(model: nn.Module, architecture: str, train_loader: DataLoader, val_loader: DataLoader, epochs: int, device: str, save_prefix: str, trials=1, mean=0, sigma=0.05) -> Tuple[nn.Module, List, List, List]:\n",
    "    '''\n",
    "    Training routine for models\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Network to train\n",
    "        architecture (str): Model type to run\n",
    "        train_loader (DataLoader): Object to load training data and labels\n",
    "        val_loader (DataLoader): Object to load validation data and labels\n",
    "        epochs (int): Number of epochs to train each model for\n",
    "        device (str): Location to execute computations over\n",
    "        save_prefix (str): Prefix to use when saving model files\n",
    "        trials (int, optional): Number of times to run and average model over\n",
    "        mean (float, optional): Mean to use for injecting gaussian noise into training\n",
    "        sigma (float, optional): Variance to use for injecting gaussian noise into training\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[nn.Module, List, List, List]: Best modle trained; training loss; validation loss; validation accuracy\n",
    "    '''\n",
    "    models = []\n",
    "    train_loss_all = []\n",
    "    val_loss_all = []\n",
    "    val_accuracy_all = []\n",
    "    for trial in tqdm(range(trials), desc='Trial:', position=0):\n",
    "        reset_weights(model)\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        val_accuracy = []\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "        best_acc = 0\n",
    "        for epoch in tqdm(range(epochs), desc='Epoch', position=1 ,leave=False):\n",
    "            model.train()\n",
    "            running_loss = 0\n",
    "            last_loss = 0\n",
    "            total_correct = 0\n",
    "            running_loss_train = 0\n",
    "            for i, data in enumerate(train_loader):\n",
    "                inputs, true_labels = data[0], data[1].to(device)\n",
    "                inputs = inputs + torch.randn(*inputs.shape)*sigma+mean\n",
    "                inputs = inputs.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs.double())\n",
    "                loss = loss_fn(outputs, true_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss_train += loss.item()\n",
    "\n",
    "                # calculate accuracy\n",
    "                pred_labels = torch.argmax(outputs, dim=1, keepdim=False)\n",
    "                # true_labels = torch.argmax(true_labels, dim=1, keepdim=False)\n",
    "                correct = (pred_labels == true_labels).float().sum()\n",
    "                total_correct += correct\n",
    "\n",
    "                if i % 10 == 9:\n",
    "                    last_loss = running_loss_train / 10\n",
    "\n",
    "            # Keep track of train loss\n",
    "            train_loss.append(last_loss)\n",
    "\n",
    "            model.train(False)\n",
    "            running_loss_val = 0.0\n",
    "            correct = 0\n",
    "            cum_samples = 0\n",
    "            for i, val_data in enumerate(val_loader):\n",
    "                inputs, true_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "                cum_samples += len(true_labels) \n",
    "                # calculate loss\n",
    "                output = model(inputs.double())\n",
    "                loss = loss_fn(output, true_labels)\n",
    "                running_loss_val += loss.item()\n",
    "\n",
    "                # calculate accuracy\n",
    "                pred_labels = torch.argmax(output, dim=1, keepdim=False)\n",
    "                # true_labels = torch.argmax(true_labels, dim=1, keepdim=False)\n",
    "                correct += (pred_labels == true_labels).float().sum()\n",
    "\n",
    "            avg_val_loss = running_loss_val / (i + 1)\n",
    "            val_loss.append(avg_val_loss)\n",
    "            if correct.cpu()/cum_samples > best_acc:\n",
    "                best_acc = correct.cpu()/cum_samples\n",
    "                torch.save(model.state_dict(), f'./weights/{save_prefix}__{architecture}_best.wts')\n",
    "            \n",
    "            val_accuracy.append(correct.cpu()/cum_samples)\n",
    "    # Copy over the model\n",
    "    models.append(model)\n",
    "    train_loss_all.append(deepcopy(train_loss))\n",
    "    val_loss_all.append(deepcopy(val_loss))\n",
    "    val_accuracy_all.append(deepcopy(val_accuracy))\n",
    "    \n",
    "    return model, np.mean(train_loss_all,axis=0), np.mean(val_loss_all,axis=0), np.mean(val_accuracy_all,axis=0)\n",
    "\n",
    "def test_accuracy(model: nn.Module, test_loader: DataLoader, device: str, le: preprocessing.LabelEncoder, prefix: str) -> None:\n",
    "    '''\n",
    "    Calculate test accuracy using held-out dataset\n",
    "    Args: \n",
    "        model (nn.Module): Model architecture to pass data through\n",
    "        test_loader (DataLoader): PyTorch dataloader with references to test data\n",
    "        device (str): Location to execute computations over\n",
    "        le (preproccessing.LabelEncoder): Object used to translate string labels into numeric indicies\n",
    "        prefix (str): Prefix to use when saving model files\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # Evaluate model on held out test set\n",
    "    model.train(False)\n",
    "    test_accuracy = []\n",
    "    running_loss_val = 0.0\n",
    "    correct = 0\n",
    "    cum_samples = 0\n",
    "    all_true = []\n",
    "    all_preds = []\n",
    "    for i, val_data in enumerate(test_loader):\n",
    "        inputs, true_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "        all_true.extend(list(true_labels.cpu()))\n",
    "        cum_samples += len(true_labels) \n",
    "        # calculate loss\n",
    "        output = model(inputs.double())\n",
    "\n",
    "        # calculate accuracy\n",
    "        pred_labels = torch.argmax(output, dim=1, keepdim=False)\n",
    "        all_preds.extend(list(pred_labels.cpu()))\n",
    "        # true_labels = torch.argmax(true_labels, dim=1, keepdim=False)\n",
    "        correct += (pred_labels == true_labels).float().sum()\n",
    "\n",
    "    print(f'Heldout Test Accuracy: {correct.cpu()/cum_samples}')\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(le.inverse_transform(all_true), le.inverse_transform(all_preds), normalize='true')\n",
    "    plt.xticks(rotation = 90) # Rotates X-Axis Ticks by 45-degrees\n",
    "    fig = disp.ax_.get_figure() \n",
    "    fig.set_figwidth(12)\n",
    "    fig.set_figheight(12)\n",
    "    plt.title('Full Data Confusion Matrix')\n",
    "    plt.savefig(f\"./figs/{prefix}_confusion_matrix.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_train_results(train_loss: List, val_loss: List, val_accuracy: List, prefix: str, descriptor: str) -> None:\n",
    "    '''\n",
    "    Plots loss and accuracy results from training a model\n",
    "    \n",
    "    Args: \n",
    "        train_loss (list): Numerical data for training loss as a function of epoch\n",
    "        val_loss (list): Numerical data for validation loss as a function of epoch\n",
    "        val_accuracy (list): Numerical data for validation accuracy over training\n",
    "        prefix (str): Prefix to use when saving model files\n",
    "        descriptor (str): Title descriptor for displaying with figures\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # View training results\n",
    "    plt.figure(1)\n",
    "    plt.plot(range(1, len(train_loss) + 1), train_loss, label=\"Train loss\")\n",
    "    plt.plot(range(1, len(train_loss) + 1), val_loss, label=\"Val loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.title(f\"{descriptor} - Train & Validation Loss\")\n",
    "    plt.savefig(f\"./figs/{prefix}_train_val_loss.png\")\n",
    "    plt.show()\n",
    "    # Accuracy\n",
    "    plt.figure(2)\n",
    "    plt.plot(range(1, len(val_accuracy) + 1), np.array(val_accuracy) * 100, label=\"Val accuracy\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy %\")\n",
    "    plt.title(f\"{descriptor} - Spectral Validation Accuracy\")\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"./figs/{prefix}_spectral_val_acc.png\")\n",
    "    plt.show()\n",
    "    \n",
    "def model_master(architecture: str, all_data: np.ndarray, labels: list, epochs: int, prefix: str, descriptor: str, trials=10, sigma=0.01, hidden_layers=(256,128)):\n",
    "    '''\n",
    "    Main function used to train spectroscopy models\n",
    "    \n",
    "    Args: \n",
    "        architecture (str): Model type to run\n",
    "        all_data (np.ndarray): 2D matrix of data to analyze\n",
    "        labels (list): Numerical categorical labels for each row of data\n",
    "        epochs (int): Number of epochs to train each model for\n",
    "        prefix (str): Prefix to use when saving model files\n",
    "        descriptor (str): Title descriptor for displaying with figures\n",
    "        trials (int, optional): Number of times to run and average model over\n",
    "        sigma (float, optional): Sigma for Gaussian noise process to inject while training\n",
    "        hidden_layers (Tuple, optional): Size of sequential hidden layers to use in network construction\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # Create model architecture\n",
    "    model, device = build_network(architecture, all_data.shape[1],len(np.unique(labels)),hidden_layers=hidden_layers)\n",
    "    # Fit labels to model\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    targets = le.fit_transform(labels)\n",
    "    np.save(f'./weights/label_encoder_class_{prefix}.npy', le.classes_)\n",
    "    # Create data splits\n",
    "    train_loader, val_loader, test_loader = train_val_test_split(all_data,targets)\n",
    "    # Train model given parameters\n",
    "    model, train_loss, val_loss, val_accuracy = train_model(model, architecture, train_loader, val_loader, epochs, device, prefix, trials=trials, sigma=sigma)\n",
    "    model.load_state_dict(torch.load(f'./weights/{prefix}__{architecture}_best.wts'))\n",
    "    model.to(device)\n",
    "    # Plot training results\n",
    "    plot_train_results(train_loss, val_loss, val_accuracy, prefix, descriptor)\n",
    "    # Calculate model statistics\n",
    "    test_accuracy(model, test_loader, device, le, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af187ae6-e25c-466f-aee9-f9aaafd37544",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, labels = generate_data_labels(readings_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad890497-982f-489c-8940-dde90829c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc631d-d3ce-4111-adec-49f7d129739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.hstack((all_data,np.gradient(all_data,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ff500-9d33-438d-9002-4e55de3d1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c76eac-a998-40c4-b896-777a75338a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_data[0,:])\n",
    "plt.plot(np.gradient(all_data,axis=1)[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222072a8-a824-45c8-8164-7859ad459e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit labels to model\n",
    "le_contents = preprocessing.LabelEncoder()\n",
    "le_containers = preprocessing.LabelEncoder()\n",
    "labels_contents = le_contents.fit_transform(labels[:,0])\n",
    "labels_containers = le_containers.fit_transform(labels[:,1])\n",
    "encoded_labels = np.vstack((labels_containers,labels_contents)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b227c5f-dba0-49cc-9340-46b5e4baa62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b0426-43c0-4022-9fdd-d65ad85eb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06310f7e-1a27-4b98-b946-e49b45473af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = train_val_test_split(all_data,encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716ed18-9dc4-40fa-8a69-076dc3eac7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,device = build_network(all_data.shape[1], len(np.unique(labels_containers)), len(np.unique(labels_contents)), hidden_layers_l1=(100,50,25), hidden_layers_l2=(100,50,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2947ff-cb99-4340-ab75-230227dcef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_loss_fn = nn.CrossEntropyLoss()\n",
    "contents_loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 500\n",
    "trials = 1\n",
    "best_acc = 0\n",
    "print(device)\n",
    "container_running_loss = []\n",
    "content_running_loss = []\n",
    "total_loss = []\n",
    "val_loss = []\n",
    "val_content_accuracy = []\n",
    "val_container_accuracy = []\n",
    "val_total_accuracy = []\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "for trial in tqdm(range(trials), desc='Trial', position=0):\n",
    "    reset_weights(model)\n",
    "    for epoch in tqdm(range(epochs), desc='Epoch', position=1 ,leave=False):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        last_loss = 0\n",
    "        total_correct = 0\n",
    "        running_loss_train = 0\n",
    "        running_loss_container = 0\n",
    "        running_loss_content = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, true_labels = data[0].to(device), data[1].to(device)\n",
    "            #inputs = inputs.to(device)\n",
    "            #true_labels = true_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            container_out, content_out = model(inputs)\n",
    "            # Calculate the loss on containers\n",
    "            container_loss = container_loss_fn(container_out, true_labels[:,0])\n",
    "            # Calculate the loss on contets\n",
    "            content_loss = contents_loss_fn(content_out, true_labels[:,1])\n",
    "            # Add the loss functions\n",
    "            loss = container_loss + content_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_train += loss.item()\n",
    "            running_loss_container += container_loss.item()\n",
    "            running_loss_content += content_loss.item()\n",
    "            # calculate accuracy\n",
    "            # pred_labels = torch.argmax(outputs, dim=1, keepdim=False)\n",
    "            # true_labels = torch.argmax(true_labels, dim=1, keepdim=False)\n",
    "            # correct = (pred_labels == true_labels).float().sum()\n",
    "            # total_correct += correct\n",
    "\n",
    "            if i % 10 == 9:\n",
    "                last_loss = running_loss_train / 10\n",
    "                last_container_loss = running_loss_container / 10\n",
    "                last_content_loss = running_loss_content / 10\n",
    "\n",
    "        # Keep track of train loss\n",
    "        total_loss.append(last_loss)\n",
    "        content_running_loss.append(last_content_loss)\n",
    "        container_running_loss.append(last_container_loss)\n",
    "\n",
    "        model.train(False)\n",
    "        running_loss_val = 0.0\n",
    "        correct_containers = 0\n",
    "        correct_contents = 0\n",
    "        correct_total = 0\n",
    "        cum_samples = 0\n",
    "        for i, val_data in enumerate(val_loader):\n",
    "            inputs, true_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "            cum_samples += len(true_labels) \n",
    "            # calculate loss\n",
    "            container_out, content_out = model(inputs)\n",
    "            # Calculate the loss on containers\n",
    "            container_loss = container_loss_fn(container_out, true_labels[:,0])\n",
    "            # Calculate the loss on contets\n",
    "            content_loss = contents_loss_fn(content_out, true_labels[:,1])\n",
    "            loss = container_loss + content_loss\n",
    "\n",
    "            running_loss_val += loss.item()\n",
    "\n",
    "            # calculate accuracy\n",
    "            pred_containers = torch.argmax(container_out, dim=1, keepdim=False)\n",
    "            pred_contents = torch.argmax(content_out, dim=1, keepdim=False)\n",
    "\n",
    "            correct_containers += (pred_containers == true_labels[:,0]).float().sum()\n",
    "            correct_contents += (pred_contents == true_labels[:,1]).float().sum()\n",
    "            correct_total += torch.logical_and(pred_contents == true_labels[:,1], pred_containers == true_labels[:,0]).float().sum()\n",
    "\n",
    "        avg_val_loss = running_loss_val / (i + 1)\n",
    "        val_loss.append(avg_val_loss)\n",
    "        if correct_total.cpu()/cum_samples > best_acc:\n",
    "            best_acc = correct_total.cpu()/cum_samples\n",
    "            torch.save(model.state_dict(), f'./weights/cont_net_best_2.wts')\n",
    "\n",
    "        val_content_accuracy.append(correct_contents.cpu()/cum_samples)\n",
    "        val_container_accuracy.append(correct_containers.cpu()/cum_samples)\n",
    "        val_total_accuracy.append(correct_total.cpu()/cum_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc9b42-9adc-4f76-91de-f935fb76abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(total_loss).reshape(trials,epochs).mean(axis=0),label='Train Loss')\n",
    "plt.plot(np.array(val_loss).reshape(trials,epochs).mean(axis=0),label='Validation Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578cf80-c78b-4d2f-9922-a37112f76aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(val_content_accuracy).reshape(trials,epochs).mean(axis=0),label='Validation Content Accuracy')\n",
    "plt.plot(np.array(val_container_accuracy).reshape(trials,epochs).mean(axis=0),label='Validation Container Accuracy')\n",
    "plt.plot(np.array(val_total_accuracy).reshape(trials,epochs).mean(axis=0),label='Validation Overall Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909a36b-6cd7-4fed-84f1-195d5b68a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on held out test set\n",
    "model.load_state_dict(torch.load(f'./weights/cont_net_best.wts'))\n",
    "model.to(device)\n",
    "model.train(False)\n",
    "correct_contents = 0\n",
    "correct_containers = 0\n",
    "correct_total = 0\n",
    "cum_samples = 0\n",
    "all_true = []\n",
    "all_preds_content = []\n",
    "all_preds_container = []\n",
    "with torch.no_grad():\n",
    "    for i, val_data in enumerate(test_loader):\n",
    "        inputs, true_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "        all_true.extend(true_labels.cpu().numpy())\n",
    "        cum_samples += len(true_labels) \n",
    "        # calculate loss\n",
    "        container_out, content_out = model(inputs)\n",
    "\n",
    "        # calculate accuracy\n",
    "        pred_containers = torch.argmax(container_out, dim=1, keepdim=False)\n",
    "        pred_contents = torch.argmax(content_out, dim=1, keepdim=False)\n",
    "        all_preds_content.extend(list(pred_contents.cpu()))\n",
    "        all_preds_container.extend(list(pred_containers.cpu()))\n",
    "        # true_labels = torch.argmax(true_labels, dim=1, keepdim=False)\n",
    "        correct_containers += (pred_containers == true_labels[:,0]).float().sum()\n",
    "        correct_contents += (pred_contents == true_labels[:,1]).float().sum()\n",
    "        correct_total += torch.logical_and(pred_contents == true_labels[:,1], pred_containers == true_labels[:,0]).float().sum()\n",
    "\n",
    "    print(f'Heldout Test Accuracy Containers: {correct_containers.cpu()/cum_samples}')\n",
    "    print(f'Heldout Test Accuracy Contents: {correct_contents.cpu()/cum_samples}')\n",
    "    print(f'Heldout Test Accuracy Overall: {correct_total.cpu()/cum_samples}')\n",
    "\n",
    "all_true = np.array(all_true)\n",
    "plt.figure()\n",
    "disp = ConfusionMatrixDisplay.from_predictions(le_containers.inverse_transform(all_true[:,0]), le_containers.inverse_transform(all_preds_container), normalize='true')\n",
    "plt.xticks(rotation = 90) # Rotates X-Axis Ticks by 45-degrees\n",
    "fig = disp.ax_.get_figure() \n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(12)\n",
    "plt.title('Container Confusion Matrix')\n",
    "plt.savefig(f\"./figs/all_containers_confusion_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "disp = ConfusionMatrixDisplay.from_predictions(le_contents.inverse_transform(all_true[:,1]), le_contents.inverse_transform(all_preds_content), normalize='true')\n",
    "plt.xticks(rotation = 90) # Rotates X-Axis Ticks by 45-degrees\n",
    "fig = disp.ax_.get_figure() \n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(12)\n",
    "plt.title('Contents Confusion Matrix')\n",
    "plt.savefig(f\"./figs/all_contents_confusion_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132d50c-b205-4cf0-b424-a4806d8f312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create calibration function\n",
    "def spectral_calibration(reading):\n",
    "    t = np.divide((reading-dark_ref), (white_ref-dark_ref), where=(white_ref-dark_ref)!=0)\n",
    "    # Handle cases where there is null division, which casts values as \"None\"\n",
    "    if np.sum(t==None) > 0:\n",
    "        print('Null readings!')\n",
    "    t[t== None] = 0\n",
    "    # Handle edge cases with large spikes in data, clip to be within a factor of the white reference to avoid skewing the model\n",
    "    t = np.clip(t,-2,2)\n",
    "    return t\n",
    "\n",
    "m = nn.Softmax(dim=1)\n",
    "# Create a dummy array here\n",
    "def eval_single(model: nn.Module, data: torch.tensor, device: str) -> None:\n",
    "    '''\n",
    "    Calculate test accuracy using held-out dataset\n",
    "    Args: \n",
    "        model (nn.Module): Model architecture to pass data through\n",
    "        test_loader (DataLoader): PyTorch dataloader with references to test data\n",
    "        device (str): Location to execute computations over\n",
    "        le (preproccessing.LabelEncoder): Object used to translate string labels into numeric indicies        \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # Evaluate model on held out test set\n",
    "    model.train(False)\n",
    "    with torch.no_grad():\n",
    "        inputs = data.to(device)\n",
    "        container_out, content_out = model(inputs)\n",
    "\n",
    "        # calculate accuracy\n",
    "        pred_containers = torch.argmax(container_out, dim=1, keepdim=False).cpu()\n",
    "        pred_contents = torch.argmax(content_out, dim=1, keepdim=False).cpu()\n",
    "        print(sorted(zip(le_containers.classes_,m(container_out).cpu().numpy()[0,:]), key=lambda x: x[1], reverse=True)[:3])\n",
    "        print(f'Predicted Container Class: {le_containers.inverse_transform(pred_containers)}')\n",
    "        print(sorted(zip(le_contents.classes_,m(content_out).cpu().numpy()[0,:]), key=lambda x: x[1], reverse=True)[:3])\n",
    "        print(f'Predicted Contents Class: {le_contents.inverse_transform(pred_contents)}')\n",
    "\n",
    "i = 1\n",
    "for f in os.listdir('./demo_data'):\n",
    "    print(i)\n",
    "    print(f.split('_')[0])\n",
    "    data = spectral_calibration(np.load(f'./demo_data/{f}')[:,:])\n",
    "    print(data.shape)\n",
    "    data = torch.Tensor(np.hstack((data,np.gradient(data,axis=1))))\n",
    "    eval_single(model,data,'cuda')\n",
    "    print('=================================')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395a457-a74a-473d-b154-d5b3f8c70fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(f'./demo_data/{f}').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce92a0f-154b-4325-b753-c825ab7add46",
   "metadata": {},
   "outputs": [],
   "source": [
    "len('IIIIIIIIIIIIII')/(14+len('IIIIIIIIII'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cd71d7-635f-4b7e-803a-1f3039692d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "len('IIIIIIIIIIIII')/(len('IIIIIIIIIIIIIIIIIIIIIIII'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b93118-8db2-440e-82dd-fe0e932e645c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
